import os
import json
import time
import random
from dotenv import load_dotenv
import google.generativeai as genai

# ==========================================
# ğŸ›‘ã€æœ€åç¡®è®¤ã€‘è¯·ç¡®è®¤ä½ çš„ VPN ç«¯å£ï¼
# Clash = 7890
# v2rayN = 10809
# ==========================================
PROXY_PORT = "33210"  # <--- å¦‚æœæŠ¥é”™ Connection Refusedï¼Œè¯·æŠŠè¿™é‡Œæ”¹æˆ 10809 è¯•è¯•

os.environ["HTTP_PROXY"] = f"http://127.0.0.1:{PROXY_PORT}"
os.environ["HTTPS_PROXY"] = f"http://127.0.0.1:{PROXY_PORT}"

load_dotenv()
GEMINI_KEY = os.getenv("GEMINI_API_KEY")

if GEMINI_KEY:
    # [CTO å…³é”®ä¿®å¤] transport='rest' 
    # å¼ºåˆ¶ä½¿ç”¨ HTTP åè®®ï¼Œé¿å… gRPC åœ¨ä»£ç†ä¸‹æŠ¥é”™
    genai.configure(api_key=GEMINI_KEY, transport='rest')

def analyze_item_with_llm(item):
    if not GEMINI_KEY: return None

    # [CTO ä¿®æ­£] ä½¿ç”¨ä½ åˆ—è¡¨é‡Œç¡®è®¤å­˜åœ¨çš„æ¨¡å‹
    # å¦‚æœ 2.0 è¿˜æ˜¯ 429ï¼Œå°±è‡ªåŠ¨é™çº§
    target_models = ['gemini-2.0-flash-lite', 'gemini-flash-latest', 'gemini-pro']
    
    for model_name in target_models:
        try:
            model = genai.GenerativeModel(model_name)
            
            prompt = f"""
            ä½ æ˜¯ AI ç§‘æŠ€åª’ä½“ç¼–è¾‘ã€‚è¯·å°†ä»¥ä¸‹å†…å®¹æ”¹å†™ä¸ºä¸­æ–‡ç®€æŠ¥ï¼š
            Title: {item['title']}
            Desc: {item['description']}
            
            è¾“å‡ºçº¯ JSON:
            {{
                "score": (0-10åˆ†),
                "summary": (ä¸€å¥è¯ä¸­æ–‡ä»‹ç»),
                "tag": (LLM/Agent/Tool)
            }}
            """
            
            response = model.generate_content(prompt)
            # æ¸…æ´— JSON
            text = response.text.strip()
            if text.startswith("```"):
                text = text.split("\n", 1)[1].rsplit("\n", 1)[0]
            
            return json.loads(text)

        except Exception as e:
            if "429" in str(e):
                print(f"   âš ï¸ Model {model_name} busy (429). Trying next...")
                time.sleep(2)
                continue # æ¢ä¸‹ä¸€ä¸ªæ¨¡å‹è¯•
            elif "404" in str(e):
                print(f"   âš ï¸ Model {model_name} not found. Trying next...")
                continue
            else:
                print(f"   âŒ Error with {model_name}: {e}")
                break # å…¶ä»–é”™è¯¯ç›´æ¥åœæ­¢

    return None

def process_data(raw_items: list) -> str:
    print(f"\nğŸ§  [Processor] Starting AI analysis on {len(raw_items)} items...")
    
    if not raw_items: return "No data."
        
    sota_items = []
    # ä»ç„¶åªè·‘å‰ 3 æ¡
    test_batch = raw_items[:3] 
    
    for i, item in enumerate(test_batch):
        print(f"   ({i+1}/{len(test_batch)}) Analyzing: {item['title']} ...", end="")
        
        analysis = analyze_item_with_llm(item)
        
        if analysis:
            print(f" Score: {analysis.get('score')}")
            item.update(analysis)
            sota_items.append(item)
        else:
            print(" Skipped")
        
        time.sleep(3)

    # --- å…œåº•é€»è¾‘ ---
    if not sota_items:
        return """
# ğŸš¨ SOTA Watch Daily (Mock Report)
> âš ï¸ AI æ¥å£æš‚æ—¶æ— æ³•è¿æ¥ï¼Œè¯·æ£€æŸ¥ VPN ç«¯å£è®¾ç½® (7890 vs 10809)ã€‚
"""

    report = f"# ğŸš¨ SOTA Watch Daily ({len(sota_items)} Updates)\n\n"
    for item in sota_items:
        report += f"### [{item.get('tag', 'AI')}] {item['title']}\n"
        report += f"**å¾—åˆ†:** {item.get('score', 0)}/10\n"
        report += f"> ğŸ’¡ {item.get('summary', 'æš‚æ— æ‘˜è¦')}\n\n"
        report += f"ğŸ”— [Original Link]({item['url']})\n"
        report += "---\n"
        
    return report

if __name__ == "__main__":
    try:
        with open("latest_data.json", "r", encoding="utf-8") as f:
            data = json.load(f)
        report = process_data(data)
        print("\n" + "="*40)
        print(report)
        print("="*40)
    except FileNotFoundError:
        print("âŒ File not found.")