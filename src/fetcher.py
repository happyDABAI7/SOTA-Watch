import os
import json
import requests
import re
import sys
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

# è¯»å– Tokenï¼Œå¦‚æœæ²¡æœ‰åˆ™ä¸º None
GITHUB_TOKEN = os.getenv("GH_TOKEN")
HF_TOKEN = os.getenv("HF_TOKEN")

# æ„é€ è¯·æ±‚å¤´
GH_HEADERS = {"Accept": "application/vnd.github.v3+json"}
if GITHUB_TOKEN:
    GH_HEADERS["Authorization"] = f"token {GITHUB_TOKEN}"

HF_HEADERS = {"Authorization": f"Bearer {HF_TOKEN}"} if HF_TOKEN else {}

NOISE_PATTERNS = [
    r"tutorial", r"course", r"learn", r"101", r"introduction", r"guide for beginners",
    r"interview", r"awesome-", r"resources", r"cheat sheet", r"roadmap"
]

def is_noise(text: str) -> bool:
    if not text: return False
    combined_pattern = "|".join(NOISE_PATTERNS)
    return bool(re.search(combined_pattern, text, re.IGNORECASE))

def fetch_github_trends():
    print("ğŸ”„ Fetching GitHub Data...")
    
    # [CTO ä¿®å¤ç‰ˆ]
    # ç®€åŒ–æŸ¥è¯¢é€»è¾‘ï¼Œé¿å… 422 è¯­æ³•é”™è¯¯
    # q: "AI topic:ai" -> æœç´¢åŒ…å« "AI" å…³é”®è¯ä¸”æ‰“äº† "ai" æ ‡ç­¾çš„é¡¹ç›®
    # è¿™æ ·ç»å¯¹ç¬¦åˆè¯­æ³•ï¼Œä¸ä¼šæŠ¥é”™
    params = {
        "q": "AI topic:ai", 
        "sort": "created",
        "order": "desc",
        "per_page": 20
    }
    
    try:
        response = requests.get("https://api.github.com/search/repositories", headers=GH_HEADERS, params=params, timeout=10)
        
        if response.status_code != 200:
            print(f"âŒ GitHub API Error: Status {response.status_code}")
            print(f"   Reason: {response.text}")
            return []
            
        items = response.json().get("items", [])
        
        results = []
        for item in items:
            desc = item.get("description") or ""
            if is_noise(item["name"]) or is_noise(desc): continue
            
            results.append({
                "source": "github",
                "title": item["full_name"],
                "url": item["html_url"],
                "description": f"â­ {item['stargazers_count']} | {desc}",
                "publish_date": item["created_at"]
            })
        print(f"âœ… GitHub: Found {len(results)} items.")
        return results
    except Exception as e:
        print(f"âŒ GitHub Connection Error: {e}")
        return []

def fetch_huggingface_trends():
    print("ğŸ”„ Fetching HF Data...")
    url = "https://huggingface.co/api/models?sort=likes&direction=-1&limit=20&full=true"
    try:
        response = requests.get(url, headers=HF_HEADERS, timeout=10)
        if response.status_code != 200:
            print(f"âš ï¸ HF API Error: {response.status_code}")
            return []
            
        models = response.json()
        results = []
        for model in models:
            if not model.get("lastModified"): continue
            desc = f"â¤ï¸ {model.get('likes', 0)} | Task: {model.get('pipeline_tag', 'Unknown')}"
            results.append({
                "source": "huggingface",
                "title": model["modelId"],
                "url": f"https://huggingface.co/{model['modelId']}",
                "description": desc,
                "publish_date": model["lastModified"]
            })
        print(f"âœ… HF: Found {len(results)} items.")
        return results
    except Exception as e:
        print(f"âŒ HF Error: {e}")
        return []

def fetch_hackernews_ai():
    print("ğŸ”„ Fetching HN Data (Top 15)...", end="")
    try:
        ids_resp = requests.get("https://hacker-news.firebaseio.com/v0/topstories.json", timeout=5)
        if ids_resp.status_code != 200:
             print("\nâŒ HN API Error")
             return []
        ids = ids_resp.json()[:15]
        
        results = []
        for item_id in ids:
            print(".", end="", flush=True)
            try:
                item_resp = requests.get(f"https://hacker-news.firebaseio.com/v0/item/{item_id}.json", timeout=3)
                if item_resp.status_code != 200: continue
                item = item_resp.json()
                if not item or "title" not in item: continue
                title = item["title"]
                if any(k in title.lower() for k in ["gpt", "llm", "ai", "transformer", "openai", "nvidia", "google"]):
                    if is_noise(title): continue
                    results.append({
                        "source": "hackernews",
                        "title": title,
                        "url": item.get("url", ""),
                        "description": f"Score: {item.get('score',0)}",
                        "publish_date": str(item.get("time"))
                    })
            except: continue
        print(f"\nâœ… HN: Found {len(results)} items.")
        return results
    except Exception as e:
        print(f"\nâŒ HN Error: {e}")
        return []

def fetch_all_data():
    data = []
    data.extend(fetch_github_trends())
    data.extend(fetch_huggingface_trends())
    data.extend(fetch_hackernews_ai())
    
    seen_urls = set()
    unique_data = []
    for item in data:
        if item["url"] not in seen_urls:
            unique_data.append(item)
            seen_urls.add(item["url"])
    return unique_data

if __name__ == "__main__":
    data = fetch_all_data()
    with open("latest_data.json", "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    print(f"ğŸ‰ Done! Saved {len(data)} items to latest_data.json")